{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0611 11:31:01.133002 14288 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 198, 198, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 99, 99, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 97, 97, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 46, 46, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               6554112   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 26)                13338     \n",
      "=================================================================\n",
      "Total params: 6,827,610\n",
      "Trainable params: 0\n",
      "Non-trainable params: 6,827,610\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get craete a pre trained model with previous weights\n",
    "\n",
    "local_weights_file = 'aslAtoZ.h5'\n",
    "\n",
    "pre_trained_model = tf.keras.models.Sequential([\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(200, 200, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(26, activation='softmax')\n",
    "])\n",
    "\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "  layer.trainable = False\n",
    "  \n",
    "pre_trained_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports for working on images\n",
    "\n",
    "import numpy as np\n",
    "#from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport numpy as np\\n#from google.colab import files\\nfrom keras.preprocessing import image\\n\\n#uploaded = files.upload()\\n#for fn in uploaded.keys():\\n\\n# predicting images\\npath = 'A_test.jpg'\\nimg = image.load_img(path, target_size=(200, 200))\\nx = image.img_to_array(img)\\nx = np.expand_dims(x, axis=0)\\n\\nimages = np.vstack([x])\\nclasses = pre_trained_model.predict(images, batch_size=10)\\nprint(path)\\n#print(classes)\\nprediction = classes.tolist()\\nprint(prediction)\\npos = prediction.index(max(prediction))\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing images using Keras Pre Processor\n",
    "'''\n",
    "import numpy as np\n",
    "#from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#uploaded = files.upload()\n",
    "#for fn in uploaded.keys():\n",
    "\n",
    "# predicting images\n",
    "path = 'A_test.jpg'\n",
    "img = image.load_img(path, target_size=(200, 200))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "images = np.vstack([x])\n",
    "classes = pre_trained_model.predict(images, batch_size=10)\n",
    "print(path)\n",
    "#print(classes)\n",
    "prediction = classes.tolist()\n",
    "print(prediction)\n",
    "pos = prediction.index(max(prediction))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "# Taking image input using OpenCV\n",
    "\n",
    "testimg = cv2.imread('E_test.jpg')\n",
    "im = Image.fromarray(testimg, 'RGB')\n",
    "# Resizing into 200x200\n",
    "im = im.resize((200,200))\n",
    "img_array = np.array(im)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "prediction = pre_trained_model.predict(img_array)\n",
    "\n",
    "print(type(prediction))\n",
    "prediction = prediction.tolist()\n",
    "print((prediction))\n",
    "\n",
    "predAns = chr(prediction[0].index(max(prediction[0])) + 65)\n",
    "print(predAns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Working on the test image\\n\\n%matplotlib inline\\n\\nimport matplotlib.pyplot as plt\\nimport matplotlib.image as mpimg\\n\\nfont = cv2.FONT_HERSHEY_SIMPLEX\\ncv2.putText(testimg,\"This is D\",(50,50),font,0.5,(255,0,0),2)\\n\\ncv2.imshow(\"testImg\", testimg)\\n\\nplt.imshow(im)\\nplt.show()\\n\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Working on the test image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "cv2.putText(testimg,\"This is D\",(50,50),font,0.5,(255,0,0),2)\n",
    "\n",
    "cv2.imshow(\"testImg\", testimg)\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n",
      "E\n",
      "F\n",
      "F\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "F\n",
      "E\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "E\n",
      "E\n",
      "E\n",
      "F\n",
      "E\n",
      "E\n",
      "E\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "F\n",
      "E\n",
      "F\n",
      "F\n",
      "F\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n"
     ]
    }
   ],
   "source": [
    "# Working on static video feed\n",
    "\n",
    "video = cv2.VideoCapture('testVideos/eVid.mp4')\n",
    "\n",
    "while(video.isOpened()):\n",
    "    \n",
    "    ret, frame = video.read()\n",
    "    #cv2.waitKey(10)\n",
    "    im = Image.fromarray(frame, 'RGB')\n",
    "    # Resizing into 200x200\n",
    "    im = im.resize((200,200))\n",
    "    img_array = np.array(im)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    prediction = pre_trained_model.predict(img_array)\n",
    "\n",
    "    #print(type(prediction))\n",
    "    prediction = prediction.tolist()\n",
    "    #print((prediction))\n",
    "\n",
    "    predAns = chr(prediction[0].index(max(prediction[0])) + 65)\n",
    "    print(predAns)\n",
    "    predText = \"This is : \" + str(predAns)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame,predText,(50,50),font,0.5,(255,0,0),2)\n",
    "\n",
    "    cv2.imshow(\"Capturing\", frame)\n",
    "    \n",
    "    key=cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
